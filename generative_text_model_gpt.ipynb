{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4211e8-7846-4119-86ee-e16425841bbf",
   "metadata": {},
   "source": [
    "# GENERATIVE TEXT MODEL USING GPT\n",
    "\n",
    "## Internship Project Submission\n",
    "\n",
    "**Name:** Sumedh Narayan Patil  \n",
    "**Technology Used:** Python, HuggingFace Transformers  \n",
    "**Model:** DistilGPT2  \n",
    "**Objective:** To build a text generation model that produces coherent paragraphs based on user prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f1c49b-5573-4dd2-acaa-fd4e83ae4bfe",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Text generation is a Natural Language Processing (NLP) task where a machine generates meaningful text based on a given prompt.\n",
    "\n",
    "In this project, we use a pre-trained Transformer-based model (DistilGPT2) to generate coherent paragraphs on specific topics provided by the user.\n",
    "\n",
    "GPT models work using probabilistic next-word prediction based on patterns learned from massive datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7e76f-7ea2-4ff5-a61f-e78cb67d00fd",
   "metadata": {},
   "source": [
    "## 2. About GPT Model\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) is based on the Transformer architecture.\n",
    "\n",
    "Key Concepts:\n",
    "- Self-Attention Mechanism\n",
    "- Tokenization\n",
    "- Pre-training on large datasets\n",
    "- Fine-tuning for downstream tasks\n",
    "\n",
    "DistilGPT2 is a smaller and faster version of GPT-2, optimized for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a992ca1e-f0fc-4011-b0e2-8c841304f6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: torch in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from transformers) (4.67.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in .\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages (from typer-slim->transformers) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (run once)\n",
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1810bd60-aeed-4b9f-b65a-33c8a79d73b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumedh Narayan Patil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Libraries Imported Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3eee9fa-0606-4162-936a-e4671717d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|████████████████████| 76/76 [00:00<00:00, 494.06it/s, Materializing param=transformer.wte.weight]\n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model...\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "print(\"Model Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b8324-6284-4db6-a6d0-9f70acf93605",
   "metadata": {},
   "source": [
    "## 3. Text Generation Process\n",
    "\n",
    "Steps:\n",
    "1. User provides a prompt.\n",
    "2. Prompt is tokenized into numerical format.\n",
    "3. Model predicts next words probabilistically.\n",
    "4. Tokens are decoded back into readable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd7af02-499f-4208-a4e3-33cd44e0fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4261598d-fe0e-49e7-a519-b93617aec38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a topic for text generation:  Future of Artificial Intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "Future of Artificial Intelligence\n",
      "The main difference between artificial intelligence and machine learning is the ability to predict, train, understand human behavior. Machines can be trained using computational techniques in order to anticipate what happens within an environment or a specific time frame; but they also have different abilities for detecting other elements that could potentially alter their natural behaviour — such as emotions (for example, emotional states) rather than emotionality (as opposed't wanting to see things from outside). But most experts believe machines will not know much about how humans behave under certain circumstances because it's hard enough to really recognize any differences when faced with various situations like violence against others: The only way AI might detect these particular phenomena would be through physical observation before being called out by someone else who may\n"
     ]
    }
   ],
   "source": [
    "user_prompt = input(\"Enter a topic for text generation: \")\n",
    "\n",
    "generated_output = generate_text(user_prompt)\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generated_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b7afe-4b4e-4ed1-8cc8-76bf9ee4b495",
   "metadata": {},
   "source": [
    "## 4. Sample Output\n",
    "\n",
    "The model successfully generated a coherent paragraph based on the provided topic.  \n",
    "The generated text demonstrates contextual understanding and logical continuation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90a31f-72b7-43ec-95b4-bb9160ed7278",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This project demonstrates the implementation of a Generative Text Model using a pre-trained GPT architecture.\n",
    "\n",
    "The model can generate coherent and contextually relevant paragraphs based on user prompts.\n",
    "\n",
    "This showcases practical application of:\n",
    "- Natural Language Processing\n",
    "- Transformer Architecture\n",
    "- Pre-trained Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896ee7f-bf46-4345-b282-225b064eb738",
   "metadata": {},
   "source": [
    "## 6. Future Improvements\n",
    "\n",
    "- Fine-tuning the model on domain-specific data\n",
    "- Deploying as a web application\n",
    "- Adding GUI interface\n",
    "- Comparing GPT with LSTM-based models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48738f08-33f6-4990-9e44-427d63858001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
